{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import etl.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add src to module search path \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "\n",
    "if os.name == \"nt\":\n",
    "    path_separator = \"\\\\\"\n",
    "else:\n",
    "    path_separator = \"/\"\n",
    "    \n",
    "module_path = f\"{parent_dir}{path_separator}src\"\n",
    "sys.path.append(module_path)\n",
    "\n",
    "# import custom module\n",
    "from etl import DataPreprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the features of the module, we perform a simplified ETL pipeline, where we extract raw data from a csv, remove duplicate rows, and store the tranformed dataset in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'..{path_separator}data{path_separator}raw{path_separator}interview_signup.csv'\n",
    "\n",
    "# class instantiation\n",
    "Pipeline_1 = DataPreprocessing(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318345, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load raw data from csv\n",
    "sep = ','\n",
    "header = 0\n",
    "dtype = {'original_product_name': str,\n",
    "         'postcode'             : str,\n",
    "         'bundesland'           : str,\n",
    "         'total_bonus'          : 'float64',\n",
    "         'order_date'           : str}\n",
    "\n",
    "Pipeline_1.load_data_from_csv(# encoding='utf-8', \n",
    "                            sep=sep,\n",
    "                            header=header,\n",
    "                            dtype=dtype)\n",
    "Pipeline_1.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_product_name', 'postcode', 'bundesland', 'total_bonus',\n",
       "       'order_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names\n",
    "Pipeline_1.df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "original_product_name     object\n",
       "postcode                  object\n",
       "bundesland                object\n",
       "total_bonus              float64\n",
       "order_date                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline_1.df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['E.ON STROM', 'E.ON STROM ÖKO', 'E.ON STROM ÖKO 24',\n",
       "       'E.ON STROM 24', 'E.ON STROM PUR', 'E.ON STROM Ã–KO',\n",
       "       'E.ON STROM 24 24 24', 'E.ON STROM 24 24', 'E.ON STROM ÖO',\n",
       "       'E.ON STROM 24 24 24 24 24 24 24'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline_1.df['original_product_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Nordrhein-Westfalen', 'Baden-Württemberg', 'Hessen', 'Berlin',\n",
       "       'Schleswig-Holstein', 'Niedersachsen', nan, 'Bayern',\n",
       "       'Rheinland-Pfalz', 'Sachsen', 'Bremen', 'Brandenburg', 'Thüringen',\n",
       "       'Saarland', 'Mecklenburg-Vorpommern', 'Hamburg', 'Sachsen-Anhalt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline_1.df['bundesland'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<class 'datetime.date'>], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline_1.df['order_date'].apply(lambda x: type(x)).unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order_date contains all strings\n",
    "Pipeline_1.df['order_date'].apply(lambda x: type(x)).unique()\n",
    "\n",
    "# convert order_date column to datetime.date\n",
    "Pipeline_1.df['order_date'] = pd.to_datetime(Pipeline_1.df['order_date'], format=\"%Y-%m-%d\")\n",
    "Pipeline_1.df['order_date'] = Pipeline_1.df['order_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 318174 entries, 0 to 318344\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count   Dtype   \n",
      "---  ------                 --------------   -----   \n",
      " 0   original_product_name  318174 non-null  category\n",
      " 1   postcode               318174 non-null  object  \n",
      " 2   bundesland             288654 non-null  category\n",
      " 3   total_bonus            318174 non-null  float64 \n",
      " 4   order_date             318174 non-null  object  \n",
      "dtypes: category(2), float64(1), object(2)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# check conversion status\n",
    "Pipeline_1.df.info()  # alt.: Pipeline_1.df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318175, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete duplicate rows\n",
    "Pipeline_1.remove_duplicate_rows()\n",
    "Pipeline_1.df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "original_product_name        0\n",
       "postcode                     0\n",
       "bundesland               29521\n",
       "total_bonus                  0\n",
       "order_date                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of missing values for each column\n",
    "columns = list(Pipeline_1.df.columns)\n",
    "Pipeline_1.missing_values(columns).sum()\n",
    "\n",
    "# alt.: my_class.df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103945                    Bayern\n",
       "72210              Niedersachsen\n",
       "149490                    Bremen\n",
       "9957           Baden-Württemberg\n",
       "178793                    Bayern\n",
       "152559           Rheinland-Pfalz\n",
       "302554    Mecklenburg-Vorpommern\n",
       "56796          Baden-Württemberg\n",
       "303500           Rheinland-Pfalz\n",
       "6383                   Thüringen\n",
       "Name: bundesland, dtype: category\n",
       "Categories (16, object): ['Baden-Württemberg', 'Bayern', 'Berlin', 'Brandenburg', ..., 'Sachsen', 'Sachsen-Anhalt', 'Schleswig-Holstein', 'Thüringen']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns logical index of all rows with missing state\n",
    "idx_missing_state = Pipeline_1.missing_values('bundesland')\n",
    "\n",
    "# Return sample of filled states\n",
    "Pipeline_1.df.loc[~idx_missing_state, 'bundesland'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288813"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of valid German states \n",
    "Pipeline_1.validate_state('bundesland').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the invalid postcode cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid postcodes:  226865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "295630    31275.0\n",
       "79595     22885.0\n",
       "24809        4564\n",
       "162159    94527.0\n",
       "107074    73550.0\n",
       "215492    44625.0\n",
       "45561     57072.0\n",
       "174003    94491.0\n",
       "164772    18437.0\n",
       "212834    73262.0\n",
       "147558    13437.0\n",
       "206993    91207.0\n",
       "303832    75181.0\n",
       "87033     17039.0\n",
       "144120    32756.0\n",
       "107416       4736\n",
       "287849    37412.0\n",
       "80465     26842.0\n",
       "132544    98553.0\n",
       "253176    89155.0\n",
       "Name: postcode, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_valid_postcodes = Pipeline_1.validate_postcode(\"postcode\", r\"^[0-9]{5}$\")\n",
    "print(\"Number of valid postcodes: \", idx_valid_postcodes.sum())\n",
    "\n",
    "# return sample of 20 invalid postcodes\n",
    "Pipeline_1.df.loc[~idx_valid_postcodes, 'postcode'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78598 entries were changed.\n",
      "\n",
      "Remaining invalid postcodes:  16610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30350     6536\n",
       "91588     8525\n",
       "109080    4177\n",
       "129002    8485\n",
       "161377    1796\n",
       "7203      4860\n",
       "202595    4509\n",
       "176469    4600\n",
       "33509     8248\n",
       "46030     4509\n",
       "42752     6484\n",
       "63876     4107\n",
       "184571    6484\n",
       "106389    9243\n",
       "316635    8491\n",
       "184465    9244\n",
       "177557    6842\n",
       "277310    4808\n",
       "9371      4519\n",
       "62982     1968\n",
       "Name: postcode, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove decimals and check again\n",
    "Pipeline_1.remove_decimals('postcode')\n",
    "\n",
    "# remaining invalid postcodes\n",
    "idx_valid_postcodes = Pipeline_1.validate_postcode('postcode')\n",
    "print(\"Remaining invalid postcodes: \", Pipeline_1.df.shape[0] - idx_valid_postcodes.sum())\n",
    "\n",
    "# show sample of 20 remaining invalid cases\n",
    "Pipeline_1.df.loc[~idx_valid_postcodes, 'postcode'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266922    92696JAVAS\n",
       "Name: postcode, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for postcodes with more than 5 digits and less than 4 digits\n",
    "idx_postcode_unequal_5 = (Pipeline_1.df['postcode'].str.len()>5) | (Pipeline_1.df['postcode'].str.len()<4)\n",
    "\n",
    "Pipeline_1.df.loc[idx_postcode_unequal_5, 'postcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid postcodes:  16609\n"
     ]
    }
   ],
   "source": [
    "# drop record with index 266922\n",
    "Pipeline_1.df.drop(index=266922, inplace=True)\n",
    "\n",
    "# remaining invalid postcodes\n",
    "idx_valid_postcodes = Pipeline_1.validate_postcode('postcode')\n",
    "print(\"Remaining invalid postcodes: \", Pipeline_1.df.shape[0] - idx_valid_postcodes.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16609 entries were changed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pad postcodes with zero from left\n",
    "Pipeline_1.zero_padding('postcode', side='left', n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining invalid postcodes:  0\n"
     ]
    }
   ],
   "source": [
    "# remaining invalid postcodes\n",
    "idx_valid_postcodes = Pipeline_1.validate_postcode('postcode')\n",
    "print(\"Remaining invalid postcodes: \", Pipeline_1.df.shape[0] - idx_valid_postcodes.sum())\n",
    "\n",
    "# have a look at cleaned column\n",
    "Pipeline_1.df[idx_valid_postcodes].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store preprocessed csv in folder data/processed\n",
    "file_path_processed = f\"..{path_separator}data{path_separator}processed{path_separator}demo_etl_module_processed.csv\"\n",
    "Pipeline_1.save_data_to_csv(file_path_processed, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
